{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re, collections\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe = pd.read_csv('all_essaysets.csv', encoding = 'latin-1')\n",
    "dataframe = pd.read_csv('training.tsv', encoding = 'latin-1', sep='\\t')\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting which set to be used 1-8\n",
    "# in order to combine them all assign set number to 9\n",
    "def select_set(dataframe,setNumber):\n",
    "    if setNumber == 9:\n",
    "        dataframe2 = dataframe[dataframe.essay_set ==1]\n",
    "        texts = dataframe2['essay']\n",
    "        scores = dataframe2['domain1_score']\n",
    "        scores = scores.apply(lambda x: (x*3)/scores.max())\n",
    "        for i in range(1,9):\n",
    "            dataframe2 = dataframe[dataframe.essay_set == i]\n",
    "            texts = texts.append(dataframe2['essay'])\n",
    "            s = dataframe2['domain1_score']\n",
    "            s = s.apply(lambda x: (x*3)/s.max())\n",
    "            scores = scores.append(s)\n",
    "    else:\n",
    "        dataframe2 = dataframe[dataframe.essay_set ==setNumber]\n",
    "        texts = dataframe2['essay']\n",
    "        scores = dataframe2['domain1_score']\n",
    "        scores = scores.apply(lambda x: (x*3)/scores.max())\n",
    "    return texts, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get histogram plot of scores and average score\n",
    "def get_hist_avg(scores,bin_count):\n",
    "    print(sum(scores)/len(scores))\n",
    "    scores.hist(bins=bin_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average word length for a text\n",
    "def avg_word_len(text):\n",
    "    clean_essay = re.sub(r'\\W', ' ', text)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    total = 0\n",
    "    for word in words:\n",
    "        total = total + len(word)\n",
    "    average = total / len(words)\n",
    "    \n",
    "    return average\n",
    "\n",
    "# word count in a given text\n",
    "def word_count(text):\n",
    "    clean_essay = re.sub(r'\\W', ' ', text)\n",
    "    return len(nltk.word_tokenize(clean_essay))\n",
    "\n",
    "# char count in a given text\n",
    "def char_count(text):\n",
    "    return len(re.sub(r'\\s', '', str(text).lower()))\n",
    "\n",
    "# sentence count in a given text\n",
    "def sent_count(text):\n",
    "    return len(nltk.sent_tokenize(text))\n",
    "\n",
    "#tokenization of texts to sentences\n",
    "def sent_tokenize(text):\n",
    "    stripped_essay = text.strip()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(stripped_essay)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", raw_sentence)\n",
    "            tokens = nltk.word_tokenize(clean_sentence)\n",
    "            tokenized_sentences.append(tokens)\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "# lemma, noun, adjective, verb, adverb count for a given text\n",
    "\n",
    "def count_lemmas(text):\n",
    "    \n",
    "    noun_count = 0\n",
    "    adj_count = 0\n",
    "    verb_count = 0\n",
    "    adv_count = 0   \n",
    "    lemmas = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_sentences = sent_tokenize(text)\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence) \n",
    "        \n",
    "        for token_tuple in tagged_tokens:\n",
    "            pos_tag = token_tuple[1]\n",
    "            \n",
    "            if pos_tag.startswith('N'): \n",
    "                noun_count += 1\n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('J'):\n",
    "                adj_count += 1\n",
    "                pos = wordnet.ADJ\n",
    "                lemmas.append(lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('V'):\n",
    "                verb_count += 1\n",
    "                pos = wordnet.VERB\n",
    "                lemmas.append(lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('R'):\n",
    "                adv_count += 1\n",
    "                pos = wordnet.ADV\n",
    "                lemmas.append(lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            else:\n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "    \n",
    "    lemma_count = len(set(lemmas))\n",
    "    \n",
    "    return noun_count, adj_count, verb_count, adv_count, lemma_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_word(text):\n",
    "    text = \"\".join([ch.lower() for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misspell_count(text):\n",
    "    spell = SpellChecker()\n",
    "    # find those words that may be misspelled\n",
    "    misspelled = spell.unknown(token_word(text))\n",
    "    #print(misspelled)\n",
    "    return len(misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(texts):\n",
    "    data = pd.DataFrame(columns=('Average_Word_Length','Sentence_Count','Word_Count',\n",
    "                                'Character_Count', 'Noun_Count','Adjective_Count',\n",
    "                                'Verb_Count', 'Adverb_Count', 'Lemma_Count' , 'Misspell_Count'\n",
    "                                 ))\n",
    "\n",
    "    data['Average_Word_Length'] = texts.apply(avg_word_len)\n",
    "    data['Sentence_Count'] = texts.apply(sent_count)\n",
    "    data['Word_Count'] = texts.apply(word_count)\n",
    "    data['Character_Count'] = texts.apply(char_count)\n",
    "    temp=texts.apply(count_lemmas)\n",
    "    noun_count,adj_count,verb_count,adverb_count,lemma_count = zip(*temp)\n",
    "    data['Noun_Count'] = noun_count\n",
    "    data['Adjective_Count'] = adj_count\n",
    "    data['Verb_Count'] = verb_count\n",
    "    data['Adverb_Count'] = adverb_count\n",
    "    data['Lemma_Count'] = lemma_count\n",
    "    data['Misspell_Count'] = texts.apply(misspell_count)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(texts,scores):\n",
    "    #create features from the texts and clean non graded essays\n",
    "    data = create_features(texts)\n",
    "    data.describe()\n",
    "    t1=np.where(np.asanyarray(np.isnan(scores)))\n",
    "    scores=scores.drop(scores.index[t1])\n",
    "    data=data.drop(scores.index[t1])\n",
    "    \n",
    "    #scaler = MinMaxScaler()\n",
    "    #data = scaler.fit_transform(data)\n",
    "\n",
    "    #train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, scores, test_size = 0.3)\n",
    "\n",
    "    #checking is there any nan cells\n",
    "    print(np.any(np.isnan(scores)))\n",
    "    print(np.all(np.isfinite(scores)))\n",
    "    return X_train, X_test, y_train, y_test, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_regression(X_train,y_train,X_test,y_test):\n",
    "    regr = LinearRegression()\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_pred = regr.predict(X_test)\n",
    "\n",
    "    # The mean squared error\n",
    "    mse=mean_squared_error(y_test, y_pred)\n",
    "    mse_per= 100*mse/3\n",
    "    print(\"Mean squared error: {}\".format(mse))\n",
    "    print(\"Mean squared error in percentage: {}\".format(mse_per))\n",
    "    #explained variance score\n",
    "    print('Variance score: {}'.format(regr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaBoost_reg(X_train,y_train,X_test,y_test):\n",
    "    #regr = RandomForestRegressor(max_depth=2, n_estimators=300)\n",
    "    #regr = SVR(gamma='scale', C=1, kernel='linear')\n",
    "    regr = AdaBoostRegressor()\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_pred = regr.predict(X_test)\n",
    "    # The mean squared error\n",
    "    mse=mean_squared_error(y_test, y_pred)\n",
    "    mse_per= 100*mse/3\n",
    "    print(\"Mean squared error: {}\".format(mse))\n",
    "    print(\"Mean squared error in percentage: {}\".format(mse_per))\n",
    "    #explained variance score\n",
    "    print('Variance score: {}'.format(regr.score(X_test, y_test)))\n",
    "\n",
    "    feature_importance = regr.feature_importances_\n",
    "\n",
    "    # make importances relative to max importance\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    feature_names = list(('Average_Word_Length','Sentence_Count','Word_Count',\n",
    "                                'Character_Count', 'Noun_Count','Adjective_Count',\n",
    "                                'Verb_Count', 'Adverb_Count', 'Lemma_Count' ,'Misspell_Count'\n",
    "                                 ))\n",
    "    feature_names = np.asarray(feature_names)\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, feature_names[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numerical scores to labels\n",
    "# (0-1.5) bad (1.5-2.3) average (2.3-3) good\n",
    "# bad:    '0'\n",
    "# average '1'\n",
    "# good    '2'\n",
    "def convert_scores(scores):\n",
    "    def mapping(x):\n",
    "        if x < np.percentile(scores,25):\n",
    "            return 0\n",
    "        elif x < np.percentile(scores,75):\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    return scores.apply(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting which set to be used 1-8\n",
    "# in order to combine them all assign set number to 9\n",
    "def select_set_classification(dataframe,setNumber):\n",
    "    if setNumber == 9:\n",
    "        dataframe2 = dataframe[dataframe.essay_set ==1]\n",
    "        texts = dataframe2['essay']\n",
    "        scores = dataframe2['domain1_score']\n",
    "        scores = scores.apply(lambda x: (x*3)/scores.max())\n",
    "        scores = convert_scores(scores)\n",
    "        for i in range(1,9):\n",
    "            dataframe2 = dataframe[dataframe.essay_set == i]\n",
    "            texts = texts.append(dataframe2['essay'])\n",
    "            s = dataframe2['domain1_score']\n",
    "            s = s.apply(lambda x: (x*3)/s.max())\n",
    "            s = convert_scores(s)\n",
    "            scores = scores.append(s)\n",
    "    else:\n",
    "        dataframe2 = dataframe[dataframe.essay_set ==setNumber]\n",
    "        texts = dataframe2['essay']\n",
    "        scores = dataframe2['domain1_score']\n",
    "        scores = scores.apply(lambda x: (x*3)/scores.max())\n",
    "        scores = convert_scores(scores)\n",
    "    return texts, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-8\n",
    "# 9:all sets combined\n",
    "texts, scores = select_set(dataframe,1)\n",
    "get_hist_avg(scores,5)\n",
    "X_train, X_test, y_train, y_test, data = data_prepare(texts,scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing for Linear Regression \\n')\n",
    "lin_regression(X_train,y_train,X_test,y_test)\n",
    "print('Testing for Adaboost Regression \\n')\n",
    "adaBoost_reg(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset selection 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-8\n",
    "# 9:all sets combined\n",
    "texts, scores = select_set_classification(dataframe,1)\n",
    "X_train, X_test, y_train, y_test, data = data_prepare(texts,scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[0.1,1,10,100,500,1000]\n",
    "for b in a:\n",
    "    clf = svm.SVC(C=b, gamma=0.00001)\n",
    "    clf.fit(X_train, y_train) \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print (b)\n",
    "    print (clf.score(X_test,y_test))\n",
    "    print (np.mean(cross_val_score(clf, X_train, y_train, cv=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=100, gamma=0.00001)\n",
    "clf.fit(X_train, y_train) \n",
    "y_pred = clf.predict(X_test)\n",
    "print('Cohen’s kappa score: {}'.format(cohen_kappa_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = data[(data[\"Character_Count\"] > 0) & (scores == 0)]\n",
    "one = data[(data[\"Character_Count\"] > 0) & (scores == 1)]\n",
    "two = data[(data[\"Character_Count\"] > 0) & (scores == 2)]\n",
    "sns.distplot(zero[\"Character_Count\"], bins=10, color='r')\n",
    "sns.distplot(one[\"Character_Count\"], bins=10, color='g')\n",
    "sns.distplot(two[\"Character_Count\"], bins=10, color='b')\n",
    "plt.title(\"Score Distribution with respect to Character_Count\",fontsize=20)\n",
    "plt.xlabel(\"Character_Count\",fontsize=15)\n",
    "plt.ylabel(\"Distribuition of the scores\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = data[(data[\"Lemma_Count\"] > 0) & (scores == 0)]\n",
    "one = data[(data[\"Lemma_Count\"] > 0) & (scores == 1)]\n",
    "two = data[(data[\"Lemma_Count\"] > 0) & (scores == 2)]\n",
    "sns.distplot(zero[\"Lemma_Count\"], bins=10, color='r')\n",
    "sns.distplot(one[\"Lemma_Count\"], bins=10, color='g')\n",
    "sns.distplot(two[\"Lemma_Count\"], bins=10, color='b')\n",
    "plt.title(\"Score Distribution with respect to lemma count\",fontsize=20)\n",
    "plt.xlabel(\"Lemma Count\",fontsize=15)\n",
    "plt.ylabel(\"Distribuition of the scores\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = data[(data[\"Sentence_Count\"] > 0) & (scores == 0)]\n",
    "one = data[(data[\"Sentence_Count\"] > 0) & (scores == 1)]\n",
    "two = data[(data[\"Sentence_Count\"] > 0) & (scores == 2)]\n",
    "sns.distplot(zero[\"Sentence_Count\"], bins=10, color='r')\n",
    "sns.distplot(one[\"Sentence_Count\"], bins=10, color='g')\n",
    "sns.distplot(two[\"Sentence_Count\"], bins=10, color='b')\n",
    "plt.title(\"Score Distribution with respect to sentence count\",fontsize=20)\n",
    "plt.xlabel(\"Sentence Count\",fontsize=15)\n",
    "plt.ylabel(\"Distribuition of the scores\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = data[(data[\"Word_Count\"] > 0) & (scores == 0)]\n",
    "one = data[(data[\"Word_Count\"] > 0) & (scores == 1)]\n",
    "two = data[(data[\"Word_Count\"] > 0) & (scores == 2)]\n",
    "sns.distplot(zero[\"Word_Count\"], bins=10, color='r')\n",
    "sns.distplot(one[\"Word_Count\"], bins=10, color='g')\n",
    "sns.distplot(two[\"Word_Count\"], bins=10, color='b')\n",
    "plt.title(\"Score Distribution with respect to word count\",fontsize=20)\n",
    "plt.xlabel(\"Word_Count\",fontsize=15)\n",
    "plt.ylabel(\"Distribuition of the scores\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = data[(data[\"Average_Word_Length\"] > 0) & (scores == 0)]\n",
    "one = data[(data[\"Average_Word_Length\"] > 0) & (scores == 1)]\n",
    "two = data[(data[\"Average_Word_Length\"] > 0) & (scores == 2)]\n",
    "sns.distplot(zero[\"Average_Word_Length\"], bins=10, color='r')\n",
    "sns.distplot(one[\"Average_Word_Length\"], bins=10, color='g')\n",
    "sns.distplot(two[\"Average_Word_Length\"], bins=10, color='b')\n",
    "plt.title(\"Score Distribution with respect to Average_Word_Length\",fontsize=20)\n",
    "plt.xlabel(\"Average_Word_Length\",fontsize=15)\n",
    "plt.ylabel(\"Distribuition of the scores\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kappa Score Reliability\n",
    "According to Cohen's original article, values ≤ 0 as indicating no agreement and 0.01–0.20 as none to slight, 0.21–0.40 as fair, 0.41– 0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1.00 as almost perfect agreement. McHugh says that many texts recommend 80% agreement as the minimum acceptable interrater agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
